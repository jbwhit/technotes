
### Basic Questions:

1. **What are some common types of evaluation metrics for machine learning models?**
    
    - What are accuracy, precision, recall, F1-score, ROC-AUC, etc.?
2. **How do you handle imbalanced datasets when evaluating a model?**
    
3. **What is the difference between a baseline model and a treatment model?**
    
    - How do you compare performance between them?
4. **What is a confusion matrix and how is it used to evaluate classification models?**
    
5. **How do you calculate and interpret sensitivity and specificity in a classification context?**
    

### Intermediate Questions:

6. **What is the significance of a p-value in hypothesis testing?**
    
    - How do you interpret the results of a hypothesis test comparing two models?
7. **Explain the concept of statistical power and how it relates to evaluating model performance.**
    
8. **What is a Type I error and a Type II error? How do they relate to model evaluation?**
    
9. **How do you use cross-validation to evaluate model performance, and why is it important?**
    
    - How does k-fold cross-validation work?
10. **What are the trade-offs between precision and recall in a planner evaluation context?**
    

### Advanced Questions:

11. **How do you evaluate model performance when different metrics are in conflict (e.g., one model is better in accuracy, but worse in recall)?**
    
12. **What is A/B testing, and how would you apply it to compare a baseline model and a treatment model?**
    
    - What statistical tests would you use to compare the results?
13. **How do you deal with metric instability over time in model evaluation?**
    
    - For example, how would you handle performance drifts in a planner model?
14. **What is the concept of a confidence interval, and how do you use it to compare models?**
    
15. **How do you evaluate models in terms of fairness?**
    
    - What are fairness metrics, and how would you use them in evaluating a treatment vs. baseline model?
16. **What is overfitting and how would you detect it in a planner evaluation model?**
    
17. **If the evaluation metrics for both models are similar but not exactly the same, how do you decide which model is better?**
    
    - How do you incorporate external factors or context into this decision?

### Deeper Questions:

18. **How do you ensure that evaluation results are not influenced by confounding variables?**
    
19. **How do you perform a Bayesian evaluation of two models?**
    
    - What priors would you consider for model comparison?
20. **Can you describe how to use bootstrapping or permutation tests for model evaluation?**
    
21. **How would you handle evaluating a planner model in an environment with noisy or sparse data?**
    
22. **What are the potential biases in evaluating models on specific metrics, and how can you correct for them?**
    
23. **How do you account for uncertainty in your model evaluation?**
    
    - How do you estimate and interpret the uncertainty in model metrics?